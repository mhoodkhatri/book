---
sidebar_position: 1
title: "Module 4: Vision-Language-Action"
description: Explore the frontier of robotics AI with VLA models. Build robots that understand natural language and translate commands into physical actions.
keywords: [VLA, vision-language-action, LLM, robotics AI, Whisper, speech, cognitive planning]
---

# Module 4: Vision-Language-Action

**VLA Models** | Weeks 11-13

Welcome to Module 4, the frontier of robotics AI. You'll learn to build robots that can **see**, **understand language**, and **act** in the physical world.

## Learning Objectives

By the end of this module, you will be able to:

- Integrate speech recognition with robotics
- Implement cognitive planning with language models
- Build end-to-end vision-language-action pipelines
- Complete a capstone project combining all course knowledge

## Module Overview

### Chapter 1: Speech to Action with Whisper
Integrate OpenAI's Whisper for speech recognition. Enable your robot to understand spoken commands and respond appropriately.

### Chapter 2: Cognitive Planning
Implement language model-based planning for complex tasks. Translate high-level instructions into robot action sequences.

### Chapter 3: Capstone Project
Apply everything you've learned in a comprehensive capstone project. Build a fully functional humanoid robot system.

## Prerequisites

Before starting this module, ensure you have:

- Completed Modules 1, 2, and 3
- Access to GPU for inference
- Understanding of transformer architectures (helpful)

## Getting Started

Begin with [Chapter 1: Speech to Action with Whisper](/docs/module-4-vla/whisper) to enable voice control for your robot.
